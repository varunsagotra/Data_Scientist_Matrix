{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Techniques :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Unit  [ e.g. feet,inch,…]\n",
    "* Magnitude [Value of feature]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Normalisation\n",
    "2. Standardisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Standardization Techniques ::\n",
    "1. Log\n",
    "2. Sqrt\n",
    "3. Standard Scalar\n",
    "4. Label Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalisation >>> helps to scale down feature between < 0 to 1 > Downscaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation (min-max Normalisation) ::\n",
    "In this approach we will scale down the values of the features betxeen < 0 to 1 >\n",
    "X norm = X - X min / X max - X min\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##### Code\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "MMS = MinMaxScaler()\n",
    "MMS.fit_transform(df[[“ ”,” ”]] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Transform features by scaling each feature to a given range ::\n",
    "- This estimator scales and translates each feature individually such that it is in the given range on the training set, \n",
    "- e.g., between zero and one. \n",
    "- This Scaler shrinks the data within the range of -1 to 1 if there are negative values. \n",
    "- We can set the range like [0,1] or [0,5] or [-1,1].\n",
    "- This Scaler responds well if the standard deviation is small and when a distribution is not Gaussian. \n",
    "- This Scaler is sensitive to outliers"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardisation \n",
    "> helps to scale down feature based on standard normal distribution [ mean = 0 & sd = 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardisation ( Z-Score Normalisation )\n",
    "- Here all the features will be transformed in such a way that it will have the properties of a standard normal distribution with mean = 0 & standard deviation = 1\n",
    "- Z = X - mean / sd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##### Code\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "SS = StandardScaler()\n",
    "SS.fit.transform(df[[“ ”,” ”]] )\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Standard Scaler assumes data is normally distributed within each feature and scales them such that the distribution centered around 0, with a standard deviation of 1.\n",
    "- Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. \n",
    "- If data is not normally distributed, this is not the best Scaler to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which one to use when :: Normalisation & Standardisation ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "` Algorithm involve ::`\n",
    "- Euclidean distance\n",
    "- Gradient decent involved ->(Parabola-curve - Find global minima point)\n",
    "\n",
    "> To find value quickly we need to scale down value ::\n",
    "\n",
    "For ML Algorithms ::\n",
    "-  KNN - Nearest neighbour\n",
    "- Linear Regression\n",
    "- Logistic Regression\n",
    "- K means clustering\n",
    "\n",
    "For Deep learning algorithms ::\n",
    "- CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Normalisation & Standardisation :: `\n",
    "@Rule of thumb we may follow here is \n",
    ">> An algorithm that computes distance or assumes normality, scales your features.\n",
    "\n",
    "@Standardisation :: \n",
    "Performs better > For all ML algorithms\n",
    "\n",
    "@Normalisation :: \n",
    "Performs better  <0 - 1> >> For deep learning algorithms\n",
    "\n",
    "- When we desire faster convergence :: Scaling is a MUST \n",
    "**like in Neural Network :: \n",
    "A neural network with saturating activation functions (e.g., sigmoid) is a good example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some examples of algorithms where feature scaling matters are::\n",
    "\n",
    "> K-nearest neighbors (KNN) with a Euclidean distance measure is sensitive to magnitudes and hence should be scaled for all features to weigh in equally.\n",
    "\n",
    "> K-Means uses the Euclidean distance measure here feature scaling matters.\n",
    "\n",
    "> Principal Component Analysis(PCA) :: Scaling is critical while performing PCA. PCA tries to get the features with maximum variance, and the variance is high for high magnitude features and skews the PCA towards high magnitude features.\n",
    "\n",
    "- We can speed up ::\n",
    "Gradient descent by scaling because θ descends quickly on small ranges and slowly on large ranges, and oscillates inefficiently down to the optimum when the variables are very uneven.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few key points to note :\n",
    "- Mean centering does not affect the covariance matrix\n",
    "- Scaling of variables does affect the covariance matrix\n",
    "- Standardizing affects the covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No need to perform scaling < algorithms >\n",
    "1.  Decision Trees, Random Forest\n",
    "2.  All boosting techniques like XG boost\n",
    "3.  Bagging techniques\n",
    "4.  Algorithms like Linear Discriminant Analysis(LDA), Naive Bayes is by design equipped to handle this and give weights to the features accordingly. \n",
    "**Performing features scaling in these algorithms may not have much effect"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
