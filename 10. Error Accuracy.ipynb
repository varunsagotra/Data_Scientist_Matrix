{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Accuracy :: Algorithm Wise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mean_absolute_error ::\n",
    "1. For Single Linear Regression use Mean Absolute Error\n",
    "2. Ridge Regression\n",
    "3. Lasso Regression"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "MAE = mean_absolute_error(ytest,y_pred)\n",
    "print(\"MAE:\",MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> R - square :: \n",
    "* Used to check >>> Goodness of best fit line\n",
    "* Square is used to negate the negative value\n",
    "* R square < 0\n",
    "* R square > 1\n",
    "\n",
    "* R square small is bad fit\n",
    "* R square large is best fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OLS >> Adjusted R2 ::\n",
    "1. For Multiple Linear Regression use OLS - Ordinary Least Square :: Adjusted R2\n",
    "2. Ridge Regression\n",
    "3. Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS - Ordinary Least Squares\n",
    "- Least square of error\n",
    "- Finds best regression line\n",
    "- Regression Line\n",
    "- Slope of sum is always zero\n",
    "Residual - Distance between Slope & Regression Line\n",
    "- RSS > Residual sum of squares\n",
    "- sum of RSS is always different number\n",
    "- if the residual sum of squares is less then its a good model\n",
    "\n",
    "Adjusted - R2 (OLS) ::\n",
    "- Penalises attributes that are not correlated \n",
    "- Adjusted r square decreases when features are not correlated otherwise if features are correlated then it increases\n",
    "- Adjusted r square value always be less than or equal to R square value\n",
    "- Adjusted R square :: Increases only when independent variables is significant & affects dependent variable"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Error - OLS >> Adjusted R2\n",
    "from statsmodels.api import OLS,add_constant\n",
    "xconstant = add_constant(xtrain)\n",
    "ols = OLS(ytrain,xconstant)\n",
    "Error = ols.fit()\n",
    "Error.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix ::\n",
    "1. Logistic Regression (FOR 2 CLASS USE CONFUSION MATRIX)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# CONFUSION MATRIX CONCEPT UNDERSTANDING ::\n",
    "    0.  1.\n",
    "0.  TN  FP\n",
    "1.  FN  TP\n",
    "\n",
    "00 & 11 >> TRUE\n",
    "01 & 10 >> FALSE\n",
    "\n",
    "00 TN\n",
    "11 TP\n",
    "10 FN\n",
    "01 FP\n",
    "\n",
    "# works only for 2 class classification : TPR, FPR \n",
    "--- For multi class use Accuracy score only\n",
    "\n",
    "# HELPS US TO UNDERSTAND MODEL BETTER\n",
    "> TPR - TRUE POSITIVE RATE\n",
    "> FPR - FALSE POSITIVE RATE\n",
    "\n",
    "# WE WILL FIND PRECISION & RECALL ONLY WHEN WE HAVE TO GIVE PREFERENCE TO ONE OVER ANOTHER\n",
    "i.e [ CASE1--> 1 OVER 0 \n",
    "                  OR \n",
    "               0 OVER 1 ]\n",
    "               \n",
    "PRECISION = \n",
    "RECALL    = \n",
    "\n",
    "# AUC BETWEEN [0 & 1] \n",
    "TRP/RECALL = TP / TP+FN  --- Y - AXIS\n",
    "FPR/RECALL = FP / TN+FP  --- X - AXIS\n",
    "[:,1] -- FOR 1\n",
    "[:,:] -- FOR BOTH (1,0)\n",
    "[:,0] -- FOR 0\n",
    "\n",
    "# USED TO COMPARE TWO BINARY Y (0,1) CLASSIFICATION PROBLEMS >> TO FIND BETTER MODEL\n",
    ">> ROC_AUC CURVE ( 0 - 1 )\n",
    "1] Y <- X1\n",
    "2] Y <- X1 & X2\n",
    "\n",
    "EXAMPLE :  0.  1.  \n",
    "       0.  7   12\n",
    "       1.  1   19    \n",
    "    \n",
    "CORRECT >>  26\n",
    "INCORRECT >> 13  --->>> PRECISION = TP/TP+FP = 19/19+12 = 0.61 \n",
    "|| [100-61 = 39] -- DEFICIT - MEANS WRONGLY PREDICTED\n",
    "RECALL = TP/TP+FN = 19/19+1 = 0.95 \n",
    "|| [100-95 = 5%] -- SCOPE FOR IMPROVEMENT IS 5%\n",
    "\n",
    "ACCURACY >>  0.66\n",
    "MISCLASSIFICATION >> 0.33\n",
    "\n",
    ":: ROC - RECIVER OPERATION CHARACTERISTICS\n",
    ":: AUC - AREA UNDER CURVE\n",
    "\n",
    "@ ROC AREA -- NOT ABLE TO UNDERSTAND\n",
    "@ AUC AREA -- ABLE TO UNDERSTAND\n",
    "@ THRESHOLD -- HOW MUCH DATA WAS EXAMINED\n",
    "\n",
    ">> ROC_AUC CASES :\n",
    "           0.   1.  \n",
    "       0.  0   12\n",
    "       1.  18   0      \n",
    "CASE - 1 > ROC_AUC AS '0' >> GOOD  * WHEN TRUE'S ARE PREDICTED AS FALSE & FALSE ARE PREDICTED AS TRUE\n",
    "           0.   1.  \n",
    "       0.  18   0\n",
    "       1.  0   12   \n",
    "CASE - 2 > ROC_AUC AS '1' >> GOOD  * PREDICTED & ACTUAL ARE 100% CORRECT\n",
    "           0.   1.  \n",
    "       0.  10   10\n",
    "       1.  10   10   \n",
    "CASE - 3 > ROC_AUC AS '0.5' >> BAD\n",
    "           0.   1.  \n",
    "       0.  10   3\n",
    "       1.  4   20   \n",
    "CASE - 4 > ROC_AUC AS '0.7' >> \n",
    "\n",
    "************************************************************************************\n",
    "\n",
    "[ F1-SCORE ] --> HARMONIC MEANS OF PRECISION & RECALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn.metrics as m\n",
    "\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "Prob = Model.predict_proba(xtest[['col1','col2']])\n",
    "pred = Prob[:,1]\n",
    "pred\n",
    "fpr,tpr,threshold = m.roc_curve(ytest,pred)\n",
    "roc_auc = m.auc(fpr,tpr)\n",
    "roc_auc\n",
    "fpr\n",
    "tpr\n",
    "threshold\n",
    "\n",
    "# Accuracy_score\n",
    "m.accuracy_score(ytest,y_pred)\n",
    "\n",
    "# Classification Report\n",
    "print(m.classification_report(ytest,y_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Score ::\n",
    "1. Logistic Regression (FOR MULTICLASS USE ACCURACY SCORE)\n",
    "2. KNeighborsClassifier\n",
    "3. DecisionTreeClassifier\n",
    "4. GaussianNB >>> Naive Bayes\n",
    "5. RandomForestClassifier"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "Score= accuracy_score(ytest,y_pred)\n",
    "print(\"Accuracy Score:\",Score)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import sklearn.metrics as sm\n",
    "sm.accuracy_score(ytest,y_pred)\n",
    "print(\"Accuracy Score:\",Score)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mean_squared_error ::\n",
    "1. KNeighborsRegressor (MEAN) \n",
    "2. DecisionTreeRegressor\n",
    "3. RandomForestRegressor"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "MSE = mean_squared_error(ytest,y_pred)\n",
    "print(\"MSE:\",MSE)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
