{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Relationship & Selection Issues ::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Relationship Types :: And :: Issues > between Independent & Dependent variables\n",
    "* 1. Correlation\n",
    "* 2. Covariance\n",
    "* 3. Collinearity / Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORRELATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CORRELATION ::\n",
    "* Correlation refers to an increase/decrease in a dependent variable with an increase/decrease in an independent variable\n",
    "* Correlation refers to the extent to which two variables have a linear relationship with each other\n",
    "* It is a statistical technique that can show whether and how strongly variables are related. \n",
    "* It is a scaled version of covariance and values ranges from -1 to +1\n",
    "\n",
    "~ FINDS:: STRENGTH OF RELATIONSHIP WITH DIRECTION \n",
    "\n",
    "*cov(X,Y)  = cov(X,Y) / sd(X) sd(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> CORRELATION is dimensionless while COVARIANCE is in units obtained by multiplying the units of the two variables. \n",
    "\n",
    "* The correlation of a variable with itself is always 1 \n",
    "  (except in the degenerate case where the two variances are zero, in that case the correlation does not exist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ways to detect Correlation between variables :: Methods ::\n",
    "1. GRAPHICAL method:\n",
    "2. Non-graphical method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. GRAPHICAL method ::\n",
    "\n",
    "- While doing bi-variate analysis between two continuous variables, we should look at scatter plot.\n",
    "- It is a nifty way to find out the relationship between two variables. \n",
    "- The pattern of scatter plot indicates the relationship between variables. \n",
    "- The relationship can be linear or non-linear.\n",
    "\n",
    "*Scatter plot shows the relationship between two variable but does not indicates the strength of relationship amongst them. \n",
    "**To find the strength of the relationship, we use statistical technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 2. Non-graphical method ::\n",
    "\n",
    "- Build the correlation matrix to understand the strength between variables. \n",
    "- Correlation varies between -1 and +1. \n",
    "\n",
    "* a. -1: Perfect negative linear correlation\n",
    "* b. +1: Perfect positive linear correlation\n",
    "* c. 0: No correlation\n",
    "* d. -1<=P(X,Y)<=0\n",
    "* e. 0<=P(X,Y)<=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a. X - decreasing :: Y - increasing in a Linear fashion >>> - ve value \n",
    "* b. X - increasing :: Y - increasing  >>> + ve value **`Drop one of the similar feature`**\n",
    "* c. NO Relationship\n",
    "* d. X - decreasing :: Y - increasing  but not in a linear fashion >>> - ve value\n",
    "* e. X - increasing :: Y - increasing  but not in a linear fashion >>> + ve value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ideal assumptions:\n",
    "* Case 1 :: High Correlation between dependent and independent variable\n",
    "* Case 2 :: Less correlation between independent variables\n",
    "\n",
    "**Generally, if the correlation between the two independent variables are high (>= 0.8) then we drop one independent variable otherwise it may lead to multi collinearity problem**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COVARIANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covariance\n",
    "* Covariance is a measure of how much two random variables change together\n",
    "* Covariance is a measure of how changes in one variable are associated with changes in a second variable. Specifically, covariance measures the degree to which two variables are linearly associated\n",
    "\n",
    "**FINDS:: DIRECTION OF RELATIONSHIP >> +VE / -VE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X - increasing :: Y - increasing  >>> covariance is + ve value`\n",
    "* If the greater values of one variable mainly correspond with the greater values of the other variable, and the same holds for the lesser values, i.e., the variables tend to show similar behaviour, the covariance is positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X - increasing :: Y - decreasing  >>> covariance is - ve value`\n",
    "* When the greater values of one variable mainly correspond to the lesser values of the other, i.e., the variables tend to show opposite behaviour, the covariance is negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Uncorrelated variables`\n",
    "* Variables whose covariance is zero are called uncorrelated variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "` cov(X,Y) = E([X-E(X)] [ Y-E(Y)] `\n",
    "* Where E(X) = mean of variable X\n",
    "* E(Y) = mean of variable Y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collinearity / Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collinearity / Multicollinearity\n",
    "> Correlation between predictor variables (or independent variables), such that they express a linear relationship in a regression model. \n",
    "\n",
    "* When predictor variables in the same regression model are correlated, they cannot independently predict the value of the dependent variable. \n",
    "\n",
    "* In other words, they explain some of the same variance in the dependent variable, which in turn reduces their statistical significance.\n",
    "\n",
    "`It may not be possible to predict multicollinearity before observing its effects on the multiple regression model, because any two of the predictor variables may have only a low degree of correlation or association`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> High Collinearity is a Problem :: [ High VIF (variance inflation factor) ]\n",
    "* It’s an association between two potential predictor variables, when there is a dramatic increase in the p value (i.e., **reduction in the significance level**) of one predictor variable when another predictor is included in the regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> VIF - Variance Inflation Factor ::\n",
    "* VIF (variance inflation factor) provides a measure of the degree of collinearity\n",
    "\n",
    "* Case1:`* Good #VIF` >> of [ 1 or 2 ] - shows essentially no collinearity \n",
    "* Case2:`* Bad #VIF`  >> of [ 20 or higher ] - shows extreme collinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Multicollinearity Issue Occurs`:: \n",
    "* Occurs when X variables are themselves related\n",
    "\n",
    "* Multicollinearity describes a situation in which more than two predictor variables are associated so that, when all are included in the model, a decrease in statistical significance is observed\n",
    "\n",
    "* Case1:-`* Bad #VIF ` >> of [ 10 or higher ] - shows extreme multicollinearity"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"Remedies/Solutions to fix ::\n",
    "\n",
    "@Option 1: Do Nothing ::\n",
    "- If model issued for prediction only\n",
    "- If correlated variables are not of particular interest to study questions\n",
    "- If correlation is not extreme\n",
    "\n",
    "@Option 2: Remove one of the correlated variables  ::\n",
    "- If variables are providing the same information\n",
    "Note:: Beware of omitted variable  bias !\n",
    "\n",
    "@Option 3: Combine the correlated variables ::\n",
    "Eg:- Include a ‘seniority’  score combining both ‘col1 & ‘col2'\n",
    "\n",
    "@Option 4: \n",
    "1. Use partial least squares OR Principal components analysis\n",
    "2. SVD (Singular value Decomposition)\n",
    "3. ML Algorithms >> Decision Trees or Random Forest"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE ENGINEERING ::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Process of using Domain Knowledge to transform raw data into informative features that represents the business problem you are trying to solve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This step involves the art and science of transforming raw data into features that better represent a pattern to the learning algorithms.\n",
    "** For example, data can be decomposed into multiple parts to capture more specific relationships, such as analyzing sales performance by the day of the week, not only the month or year.\n",
    "- In this situation, segregating the day as a separate categorical value from the date (e.g. “Mon; 06.19.2017”) may provide the algorithm with more relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection Technique ::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> `Filter Methods` :: Check relevance of an attribute\n",
    "1. IG - Information Gain\n",
    "2. Chi-Square Test\n",
    "3. Correlation coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> `Wrapper Methods` :: Check Usefulness of an attribute\n",
    "1. Recursive feature elimination\n",
    "2. Genetic Algorithms\n",
    "\n",
    "** Computationally very expensive if no. of features exceeds the limit Overfitting Problem occurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> `Embedded Methods` :: Check Usefulness of an attribute\n",
    "\n",
    "**Decision Tree**\n",
    "\n",
    "* 1. Computationally less expensive,\n",
    "* 2. Less chance of overfitting after pruning"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
