{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Techniques ::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Convert Categorical data to Numeric data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ordinal Encoding\n",
    "* When order/rank of categories is important use Ordinal Encoding\n",
    "* Rearrange categories based on rank\n",
    "    \n",
    "> Nominal Encoding\n",
    "* When order of categories is not important use Nominal Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinal Encoding Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Ordinal Encoding Techniques ::`\n",
    "> For Binary categorical features use Label Encoding \n",
    "  i.e. features having < 2 > categories only\n",
    "  \n",
    "> Target Guided Ordinal Encoding : <Many unique categories(Feature)>\n",
    "* 1. Directly compare feature categories on by one with target value & find mean of each category\n",
    "* 2. Assign calculated mean value to each respective category as rank\n",
    "* 3. Based on rank assign labels using Label Encoding technique "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "> Label Encoding\n",
    " \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit_transform(df1.col)\n",
    "df1['New_col']= le.fit_transform(df1.col)\n",
    "df1[’New_col']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nominal Encoding Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Nominal Encoding Techniques ::`\n",
    "\n",
    "1. One Hot Encoding  \n",
    "** Use for Low cardinality features]\n",
    "** For Multi categorical features use One hot Encoding \n",
    "    i.e. features having more than’ ‘2’ categories\n",
    "** If no. of unique categories are more then Find top 10 most repeated categories and perform encoding only on selected categories(KDD Orange) & protect from dummy variable trap\n",
    "\n",
    "2. Mean Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Disadvantage of One hot encoding :: \n",
    "- If no. of unique categories in a column/feature are more then it will lead to curse of dimensionality i.e. High cardinality problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot Encoding code"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "> One hot Encoding code :: \n",
    "* df2 = df1.join(pd.get_dummies(df1['col'],drop_first=True))\n",
    "  df2.head(2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "> Find top 10 most repeated categories :: Code\n",
    "* Function :: \n",
    "* Get whole set of dummy variables, for all the categorical variables\n",
    "\n",
    "def one_hot_top_x( df, variable,top_x_labels):\n",
    "`function to create the dummy variables for the most frequent labels,we can vary the number of most frequent labels that we encode`\n",
    "*       for label in top_x_labels:\n",
    "*            df[variable+’_’+label] = np.where(data[variable]==label,1,0)\n",
    "            \n",
    "` read the data again`\n",
    "\n",
    "* Data = pd.read_csv(‘ url ’, usec`ls=[‘col1 ’,’col2 ’,’ ’,’ ’,’ ’])\n",
    "\n",
    "`encode ‘col1’ into the 10 most frequent categories`\n",
    "\n",
    "* one_hot_top_x(data,’col1’,top_10)\n",
    "* Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `Advantage of selecting Top 10 most frequent categories ::`\n",
    "* 1. Straightforward to implement\n",
    "* 2. Does not require hrs. of variable exploration\n",
    "* 3. Does not expand massively the feature space (no. of columns in the dataset)\n",
    "\n",
    "> ` Disadvantages ::`\n",
    "* 1. Does not add any information that may make the variable more predictive\n",
    "* 2. Does not keep the information of the ignored labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Mean Encoding : <Scenario e.g. Pin code(Feature)>\n",
    "1. Directly compare feature categories on by one with target value & find mean of each category\n",
    "2. Replace calculated mean value to each respective category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy Variable trap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dummy Variable trap ::\n",
    "- Delete one of the converted category column as it can be predicted if all other columns have 0 value by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curse of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Curse of dimensionality ::\n",
    "- If feature categories are increased above threshold limit then model is going to perform poorly i.e. Accuracy decreases\n",
    "- It happens because of unnecessary feature selection/addition"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
